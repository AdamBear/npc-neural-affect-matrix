# ğŸ“‹ ONNXæ¨¡å‹è®­ç»ƒæ­¥éª¤æŒ‡å—

## ğŸ“Š æ¨¡å‹è¾“å…¥è¾“å‡ºæ ¼å¼

æ ¹æ®ä»£ç  `src/modules/emotion/predictor.rs:209-262`ï¼Œå½“å‰æ¨¡å‹è§„æ ¼ï¼š

### è¾“å…¥æ ¼å¼
- `input_ids`: å½¢çŠ¶ `[1, 512]`ï¼Œç±»å‹ `int64`ï¼ˆtokenizedæ–‡æœ¬IDï¼‰
- `attention_mask`: å½¢çŠ¶ `[1, 512]`ï¼Œç±»å‹ `int64`ï¼ˆæ³¨æ„åŠ›æ©ç ï¼‰
- æœ€å¤§æ–‡æœ¬é•¿åº¦ï¼š512 tokens
- Tokenizerï¼šåŸºäºBERTçš„tokenizerï¼ˆ`tokenizer.json`ï¼‰

### è¾“å‡ºæ ¼å¼
- å½¢çŠ¶ï¼š`[1, 2]`ï¼Œç±»å‹ `float32`
- `[0, 0]`ï¼šValenceï¼ˆæƒ…æ„Ÿæ„‰æ‚¦åº¦ï¼ŒèŒƒå›´ -1.0 åˆ° +1.0ï¼‰
- `[0, 1]`ï¼šArousalï¼ˆæƒ…æ„Ÿæ¿€æ´»åº¦ï¼ŒèŒƒå›´ -1.0 åˆ° +1.0ï¼‰

---

## ğŸ”§ è®­ç»ƒæ­¥éª¤

### 1. å‡†å¤‡è®­ç»ƒæ•°æ®é›†

æ¨èä½¿ç”¨é¡¹ç›®æä¾›çš„æ•°æ®é›†æˆ–ç±»ä¼¼æ ¼å¼ï¼š
- **å®˜æ–¹æ•°æ®é›†**: [NPC Valence-Arousal Dataset](https://huggingface.co/datasets/Mavdol/NPC-Valence-Arousal) ï¼ˆ70K+æ¸¸æˆå¯¹è¯ï¼‰

æ•°æ®æ ¼å¼åº”ä¸ºï¼š
```json
[
  {
    "text": "Thank you for saving my life!",
    "valence": 0.85,
    "arousal": 0.45
  },
  {
    "text": "How dare you attack my village!",
    "valence": -0.78,
    "arousal": 0.82
  }
]
```

### 2. é€‰æ‹©åŸºç¡€æ¨¡å‹æ¶æ„

å½“å‰é¡¹ç›®ä½¿ç”¨çš„æ˜¯**BERT-basedæ¶æ„**ã€‚æ¨èé€‰é¡¹ï¼š
- **DistilBERT**: è½»é‡çº§ï¼Œé€‚åˆå®æ—¶æ¸¸æˆï¼ˆæ¨èï¼‰
- **BERT-base**: æ ‡å‡†é€‰æ‹©
- **RoBERTa**: æ›´é«˜ç²¾åº¦ä½†æ›´å¤§

### 3. è®­ç»ƒç¯å¢ƒæ­å»º

```bash
# åˆ›å»ºPythonè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# å®‰è£…ä¾èµ–
pip install torch transformers datasets onnx onnxruntime scikit-learn
```

### 4. è®­ç»ƒè„šæœ¬ç¤ºä¾‹

```python
import torch
import torch.nn as nn
from transformers import AutoModel, AutoTokenizer, Trainer, TrainingArguments
from datasets import load_dataset
import onnx

# 1. å®šä¹‰æ¨¡å‹æ¶æ„
class ValenceArousalModel(nn.Module):
    def __init__(self, base_model_name="distilbert-base-uncased"):
        super().__init__()
        self.bert = AutoModel.from_pretrained(base_model_name)
        # è¾“å‡º2ç»´ï¼š[valence, arousal]
        self.regressor = nn.Linear(self.bert.config.hidden_size, 2)
        self.tanh = nn.Tanh()  # é™åˆ¶è¾“å‡ºåˆ° [-1, 1]

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled = outputs.last_hidden_state[:, 0, :]  # [CLS] token
        predictions = self.regressor(pooled)
        return self.tanh(predictions)

# 2. åŠ è½½æ•°æ®é›†
dataset = load_dataset("Mavdol/NPC-Valence-Arousal")  # æˆ–æ‚¨è‡ªå·±çš„æ•°æ®
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

def preprocess(examples):
    tokens = tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=512
    )
    tokens["labels"] = [[ex["valence"], ex["arousal"]] for ex in examples]
    return tokens

train_dataset = dataset["train"].map(preprocess, batched=True)

# 3. è®­ç»ƒé…ç½®
model = ValenceArousalModel()

training_args = TrainingArguments(
    output_dir="./npc-emotion-model",
    num_train_epochs=5,
    per_device_train_batch_size=16,
    learning_rate=2e-5,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir="./logs",
    save_strategy="epoch"
)

# 4. è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼ˆMSE for regressionï¼‰
def compute_loss(model, inputs, return_outputs=False):
    outputs = model(**inputs)
    labels = inputs.pop("labels")
    loss = nn.MSELoss()(outputs, labels)
    return (loss, outputs) if return_outputs else loss

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    compute_loss=compute_loss
)

# 5. å¼€å§‹è®­ç»ƒ
trainer.train()

# 6. ä¿å­˜PyTorchæ¨¡å‹
model.save_pretrained("./trained_model")
tokenizer.save_pretrained("./trained_model")
```

### 5. è½¬æ¢ä¸ºONNXæ ¼å¼

```python
import torch
import onnx

# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
model = ValenceArousalModel()
model.load_state_dict(torch.load("./trained_model/pytorch_model.bin"))
model.eval()

# å‡†å¤‡è™šæ‹Ÿè¾“å…¥ï¼ˆç”¨äºONNXå¯¼å‡ºï¼‰
dummy_input_ids = torch.randint(0, 30522, (1, 512))
dummy_attention_mask = torch.ones((1, 512), dtype=torch.long)

# å¯¼å‡ºä¸ºONNX
torch.onnx.export(
    model,
    (dummy_input_ids, dummy_attention_mask),
    "model.onnx",
    input_names=["input_ids", "attention_mask"],
    output_names=["output"],
    dynamic_axes={
        "input_ids": {0: "batch_size"},
        "attention_mask": {0: "batch_size"}
    },
    opset_version=14
)

# éªŒè¯ONNXæ¨¡å‹
onnx_model = onnx.load("model.onnx")
onnx.checker.check_model(onnx_model)
print("âœ… ONNXæ¨¡å‹éªŒè¯æˆåŠŸ!")
```

### 6. æµ‹è¯•ONNXæ¨¡å‹

```python
import onnxruntime as ort
import numpy as np

# åŠ è½½ONNX Runtimeä¼šè¯
session = ort.InferenceSession("model.onnx")

# æµ‹è¯•è¾“å…¥
text = "Thank you for saving my village!"
tokens = tokenizer(text, padding="max_length", truncation=True, max_length=512, return_tensors="np")

# æ¨ç†
outputs = session.run(
    None,
    {
        "input_ids": tokens["input_ids"].astype(np.int64),
        "attention_mask": tokens["attention_mask"].astype(np.int64)
    }
)

valence, arousal = outputs[0][0]
print(f"Valence: {valence:.2f}, Arousal: {arousal:.2f}")
```

---

## ğŸ”„ é›†æˆåˆ°é¡¹ç›®ä¸­

è®­ç»ƒå®Œæˆåï¼Œæ›¿æ¢é¡¹ç›®ä¸­çš„æ¨¡å‹ï¼š

### 1. å‡†å¤‡æ¨¡å‹æ–‡ä»¶
- `model.onnx`ï¼ˆONNXæ¨¡å‹ï¼‰
- `tokenizer.json`ï¼ˆHuggingFaceæ ¼å¼ï¼‰
- `config.json`ï¼ˆå¯é€‰é…ç½®ï¼‰

### 2. æ”¾ç½®åˆ°ç¼“å­˜ç›®å½•
```bash
# é¡¹ç›®ä¼šè‡ªåŠ¨åœ¨ä»¥ä¸‹ä½ç½®æŸ¥æ‰¾
target/release/npc_models_cache/NPC-Prediction-Model-v0.0.1/
â”œâ”€â”€ model.onnx
â”œâ”€â”€ tokenizer.json
â”œâ”€â”€ config.json
â””â”€â”€ version.txt
```

### 3. æ›´æ–°ç‰ˆæœ¬å·ï¼ˆå¯é€‰ï¼‰
ä¿®æ”¹ `src/modules/emotion/predictor.rs:71`ï¼š
```rust
const MODEL_VERSION: &'static str = "v0.0.2"; // æ‚¨çš„æ–°ç‰ˆæœ¬
```

---

## ğŸ“ˆ è®­ç»ƒä¼˜åŒ–å»ºè®®

### 1. æ•°æ®å¢å¼º
- åŒä¹‰è¯æ›¿æ¢
- å›è¯‘ï¼ˆback-translationï¼‰
- æ·»åŠ æ¸¸æˆç‰¹å®šé¢†åŸŸæ•°æ®

### 2. è¶…å‚æ•°è°ƒä¼˜
- Learning rate: `1e-5` åˆ° `5e-5`
- Batch size: `16` æˆ– `32`
- Epochs: `3-10`ï¼ˆæ ¹æ®æ•°æ®é›†å¤§å°ï¼‰

### 3. è¯„ä¼°æŒ‡æ ‡
- MSEï¼ˆå‡æ–¹è¯¯å·®ï¼‰
- MAEï¼ˆå¹³å‡ç»å¯¹è¯¯å·®ï¼‰
- Pearsonç›¸å…³ç³»æ•°

### 4. æ¨¡å‹å‹ç¼©ï¼ˆå¯é€‰ï¼‰
- é‡åŒ–ï¼ˆINT8ï¼‰ï¼šä½¿ç”¨ `onnxruntime.quantization`
- è’¸é¦ï¼ˆDistillationï¼‰ï¼šä½¿ç”¨æ›´å°çš„å­¦ç”Ÿæ¨¡å‹

---

## ğŸ“š ç›¸å…³èµ„æº

- **å®˜æ–¹æ¨¡å‹**: [HuggingFace Model Hub](https://huggingface.co/Mavdol/NPC-Valence-Arousal-Prediction-ONNX)
- **æ•°æ®é›†**: [HuggingFace Datasets](https://huggingface.co/datasets/Mavdol/NPC-Valence-Arousal)
- **ç†è®ºåŸºç¡€**: Russell's Circumplex Model of Affect
- **ONNXæ–‡æ¡£**: https://onnx.ai/
- **å½“å‰é¡¹ç›®**: https://github.com/mavdol/npc-neural-affect-matrix

---

## ğŸ¯ å¿«é€Ÿå¼€å§‹æ£€æŸ¥æ¸…å•

- [ ] å‡†å¤‡æˆ–ä¸‹è½½è®­ç»ƒæ•°æ®é›†
- [ ] å®‰è£…Pythonä¾èµ–ï¼ˆtorch, transformers, onnxç­‰ï¼‰
- [ ] é€‰æ‹©åŸºç¡€æ¨¡å‹æ¶æ„ï¼ˆæ¨èDistilBERTï¼‰
- [ ] è¿è¡Œè®­ç»ƒè„šæœ¬
- [ ] è½¬æ¢ä¸ºONNXæ ¼å¼
- [ ] éªŒè¯ONNXæ¨¡å‹è¾“å…¥è¾“å‡º
- [ ] æµ‹è¯•æ¨ç†æ€§èƒ½
- [ ] é›†æˆåˆ°é¡¹ç›®ä¸­
- [ ] æ›´æ–°ç‰ˆæœ¬å·å’Œæ–‡æ¡£

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **å†…å­˜è¦æ±‚**: è®­ç»ƒBERTç±»æ¨¡å‹è‡³å°‘éœ€è¦8GB RAMï¼Œæ¨è16GB+
2. **GPUæ¨è**: ä½¿ç”¨CUDAåŠ é€Ÿå¯å¤§å¹…å‡å°‘è®­ç»ƒæ—¶é—´
3. **æ•°æ®è´¨é‡**: æ¨¡å‹æ€§èƒ½é«˜åº¦ä¾èµ–è®­ç»ƒæ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§
4. **ç‰ˆæœ¬å…¼å®¹**: ç¡®ä¿ONNX opsetç‰ˆæœ¬ä¸onnxruntimeç‰ˆæœ¬å…¼å®¹
5. **æµ‹è¯•è¦†ç›–**: åœ¨å¤šç§æ–‡æœ¬è¾“å…¥ä¸Šæµ‹è¯•æ¨¡å‹ï¼Œç¡®ä¿è¾“å‡ºåœ¨[-1, 1]èŒƒå›´å†…
